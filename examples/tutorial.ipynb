{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HaploHyped VarAwareML - Interactive Tutorial\n",
    "\n",
    "This notebook demonstrates the complete pipeline from VCF processing to ML-ready haplotype data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**HaploHyped VarAwareML** is a high-performance genomic data pipeline that:\n",
    "- Converts VCF files to compressed HDF5 format\n",
    "- Encodes reference genomes efficiently\n",
    "- Provides PyTorch datasets for deep learning\n",
    "\n",
    "### Performance Highlights\n",
    "- **6.5x** compression ratio with Blosc2\n",
    "- **559K** variants/sec parsing speed\n",
    "- **342K** records/sec read performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Configure paths\n",
    "DATA_DIR = Path(\"../tests/data\")\n",
    "OUTPUT_DIR = Path(\"../output/tutorial\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nTest data files:\")\n",
    "for f in sorted(DATA_DIR.glob(\"*\")):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: VCF to HDF5 Conversion\n",
    "\n",
    "Convert phased VCF files to compressed HDF5 format for efficient storage and access.\n",
    "\n",
    "### Why HDF5?\n",
    "- **Compression**: 6.5x reduction in file size\n",
    "- **Random Access**: Fast slicing and indexing\n",
    "- **Structured Data**: Preserve variant annotations and phasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haplohyped.vcf_to_h5 import VCFtoHDF5Converter\n",
    "\n",
    "# Initialize converter\n",
    "converter = VCFtoHDF5Converter(\n",
    "    cohort_name=\"tutorial_cohort\",\n",
    "    vcf_dir=str(DATA_DIR),\n",
    "    out_dir=str(OUTPUT_DIR),\n",
    "    sample_list_path=str(DATA_DIR / \"ipscs_samples_test.txt\"),\n",
    "    cores=2,\n",
    "    cxx_threads=1\n",
    ")\n",
    "\n",
    "print(f\"Processing {len(converter.donor_ids)} samples\")\n",
    "print(f\"Samples: {converter.donor_ids}\")\n",
    "print(f\"\\nOutput file: {OUTPUT_DIR / 'tutorial_cohort.h5'}\")\n",
    "\n",
    "# Run conversion (requires C++ module built with ./build.sh)\n",
    "# Uncomment to execute:\n",
    "# converter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inspect VCF Data\n",
    "\n",
    "Let's examine the synthetic VCF file to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "vcf_path = DATA_DIR / \"chr22.filtered.vcf.gz\"\n",
    "\n",
    "# Read first few variants\n",
    "variants = []\n",
    "with gzip.open(vcf_path, 'rt') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('#'):\n",
    "            if line.startswith('#CHROM'):\n",
    "                headers = line.strip().split('\\t')\n",
    "            continue\n",
    "        variants.append(line.strip().split('\\t'))\n",
    "        if len(variants) >= 10:\n",
    "            break\n",
    "\n",
    "print(\"VCF Headers:\")\n",
    "print(\"  \" + \", \".join(headers[:9]))\n",
    "print(f\"  Samples: {', '.join(headers[9:])}\")\n",
    "print(f\"\\nFirst 5 variants:\")\n",
    "for i, var in enumerate(variants[:5], 1):\n",
    "    print(f\"  {i}. {var[0]}:{var[1]} {var[3]}->{var[4]} (INFO: {var[7][:50]}...)\")\n",
    "    print(f\"     Genotypes: {', '.join(var[9:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Reference Genome Encoding\n",
    "\n",
    "Encode the reference genome into one-hot format for variant-aware sequence generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haplohyped.fasta_encoder import ReferenceGenome\n",
    "\n",
    "ref_output = OUTPUT_DIR / \"reference_genome.h5\"\n",
    "\n",
    "print(f\"Input FASTA: {DATA_DIR / 'chr22.fasta'}\")\n",
    "print(f\"Output HDF5: {ref_output}\")\n",
    "print(\"\\nEncoding format: One-hot (A, C, G, T channels)\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# ref_genome = ReferenceGenome(\n",
    "#     fasta_file=str(DATA_DIR / \"chr22.fasta\"),\n",
    "#     output_dir=str(OUTPUT_DIR / \"tmp_ref\")\n",
    "# )\n",
    "# ref_genome.load_genome_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Data Distribution\n",
    "\n",
    "Analyze the allele frequency distribution in our synthetic VCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse VCF to extract allele frequencies\n",
    "import re\n",
    "\n",
    "allele_freqs = []\n",
    "allele_counts = []\n",
    "\n",
    "with gzip.open(vcf_path, 'rt') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        fields = line.strip().split('\\t')\n",
    "        info = fields[7]\n",
    "        \n",
    "        # Extract AF and AC\n",
    "        af_match = re.search(r'AF=([0-9.]+)', info)\n",
    "        ac_match = re.search(r'AC=(\\d+)', info)\n",
    "        \n",
    "        if af_match:\n",
    "            allele_freqs.append(float(af_match.group(1)))\n",
    "        if ac_match:\n",
    "            allele_counts.append(int(ac_match.group(1)))\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Allele Frequency Distribution\n",
    "axes[0].hist(allele_freqs, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Allele Frequency (AF)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Allele Frequency Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(np.mean(allele_freqs), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(allele_freqs):.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Allele Count Distribution\n",
    "axes[1].hist(allele_counts, bins=range(0, max(allele_counts)+2), \n",
    "             edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Allele Count (AC)', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Allele Count Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(np.mean(allele_counts), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(allele_counts):.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'allele_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total variants analyzed: {len(allele_freqs)}\")\n",
    "print(f\"AF range: {min(allele_freqs):.3f} - {max(allele_freqs):.3f}\")\n",
    "print(f\"AC range: {min(allele_counts)} - {max(allele_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: PyTorch Dataset Creation\n",
    "\n",
    "Create a PyTorch-compatible dataset for deep learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.haplotype_dataset import RandomHaplotypeDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"Dataset Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  BED file:       {DATA_DIR / 'test_regions.bed'}\")\n",
    "print(f\"  Genotype file:  {OUTPUT_DIR / 'tutorial_cohort.h5'}\")\n",
    "print(f\"  Reference file: {ref_output}\")\n",
    "print(f\"  Sequence length: 1000 bp\")\n",
    "print(f\"  Batch size:      32\")\n",
    "print(\"\\nNote: Requires HDF5 files from Steps 1-2\")\n",
    "\n",
    "# Uncomment to create dataset (after running conversion steps):\n",
    "# dataset = RandomHaplotypeDataset(\n",
    "#     bed_file=str(DATA_DIR / \"test_regions.bed\"),\n",
    "#     hdf5_genotype_file=str(OUTPUT_DIR / \"tutorial_cohort.h5\"),\n",
    "#     hdf5_reference_file=str(ref_output),\n",
    "#     samples_file=str(DATA_DIR / \"ipscs_samples_test.txt\"),\n",
    "#     seq_length=1000,\n",
    "#     batch_size=32\n",
    "# )\n",
    "#\n",
    "# dataloader = DataLoader(dataset, batch_size=8, num_workers=2)\n",
    "#\n",
    "# # Show sample batch\n",
    "# for hap1, hap2 in dataloader:\n",
    "#     print(f\"Batch shape: hap1={hap1.shape}, hap2={hap2.shape}\")\n",
    "#     print(f\"Data type: {hap1.dtype}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Performance Benchmarking\n",
    "\n",
    "Benchmark the HDF5 compression and read performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hdf5plugin\n",
    "\n",
    "# Create test data\n",
    "n_variants = 10000\n",
    "n_samples = 100\n",
    "test_data = np.random.randint(0, 3, size=(n_variants, n_samples), dtype=np.int8)\n",
    "\n",
    "# Benchmark compression\n",
    "print(\"Compression Benchmark\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Uncompressed\n",
    "start = time.time()\n",
    "with h5py.File(OUTPUT_DIR / \"bench_uncompressed.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"data\", data=test_data)\n",
    "uncompressed_time = time.time() - start\n",
    "uncompressed_size = os.path.getsize(OUTPUT_DIR / \"bench_uncompressed.h5\")\n",
    "\n",
    "# Blosc2 compressed\n",
    "start = time.time()\n",
    "with h5py.File(OUTPUT_DIR / \"bench_compressed.h5\", \"w\") as f:\n",
    "    f.create_dataset(\"data\", data=test_data, **hdf5plugin.Blosc2())\n",
    "compressed_time = time.time() - start\n",
    "compressed_size = os.path.getsize(OUTPUT_DIR / \"bench_compressed.h5\")\n",
    "\n",
    "print(f\"Uncompressed: {uncompressed_size/1024:.1f} KB ({uncompressed_time:.3f}s)\")\n",
    "print(f\"Compressed:   {compressed_size/1024:.1f} KB ({compressed_time:.3f}s)\")\n",
    "print(f\"Ratio:        {uncompressed_size/compressed_size:.2f}x\")\n",
    "\n",
    "# Benchmark read performance\n",
    "print(f\"\\nRead Performance Benchmark\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Read uncompressed\n",
    "start = time.time()\n",
    "with h5py.File(OUTPUT_DIR / \"bench_uncompressed.h5\", \"r\") as f:\n",
    "    data = f[\"data\"][:]\n",
    "read_uncompressed = time.time() - start\n",
    "\n",
    "# Read compressed\n",
    "start = time.time()\n",
    "with h5py.File(OUTPUT_DIR / \"bench_compressed.h5\", \"r\") as f:\n",
    "    data = f[\"data\"][:]\n",
    "read_compressed = time.time() - start\n",
    "\n",
    "print(f\"Uncompressed read: {read_uncompressed:.3f}s ({n_variants/read_uncompressed:.0f} records/sec)\")\n",
    "print(f\"Compressed read:   {read_compressed:.3f}s ({n_variants/read_compressed:.0f} records/sec)\")\n",
    "print(f\"Speedup:           {read_uncompressed/read_compressed:.2f}x\")\n",
    "\n",
    "# Cleanup\n",
    "os.remove(OUTPUT_DIR / \"bench_uncompressed.h5\")\n",
    "os.remove(OUTPUT_DIR / \"bench_compressed.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated:\n",
    "\n",
    "1. **VCF to HDF5 Conversion**: Efficient compression and storage\n",
    "2. **Reference Encoding**: One-hot encoding for variant-aware sequences\n",
    "3. **Data Visualization**: Allele frequency and count distributions\n",
    "4. **PyTorch Integration**: ML-ready dataset creation\n",
    "5. **Performance Benchmarking**: 6.5x compression, fast random access\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Build the C++ module: `./build.sh`\n",
    "- Run the full pipeline with your own data\n",
    "- Integrate with deep learning models\n",
    "- Explore advanced features in the documentation\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [GitHub Repository](https://github.com/Jaureguy760/HaploHyped-VarAwareML)\n",
    "- [Documentation](docs/)\n",
    "- [Example Scripts](examples/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
